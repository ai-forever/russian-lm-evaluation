{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel, Seq2SeqTrainer, Seq2SeqTrainingArguments, T5ForConditionalGeneration, GPT2Tokenizer\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_auc_score, f1_score, accuracy_score\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR\n",
    "from transformers.models.bert.modeling_bert import BertPreTrainedModel\n",
    "from transformers.models.deberta_v2.modeling_deberta_v2 import DebertaV2PreTrainedModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from transformers import Adafactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('detox_data/train.tsv', sep='\\t')\n",
    "val_df = pd.read_csv('detox_data/dev.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict()\n",
    "dataset['train'] = Dataset.from_pandas(train_df[['toxic_comment', 'neutral_comment1']])\n",
    "dataset['validation'] = Dataset.from_pandas(val_df[['toxic_comment', 'neutral_comment1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'ai-forever/FRED-T5-large'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, truncation_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['toxic_comment', 'neutral_comment1'],\n",
       "        num_rows: 6948\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['toxic_comment', 'neutral_comment1'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_features(examples):\n",
    "    tokenized_examples = tokenizer(\n",
    "        ['<LM>' +x for x in examples[\"toxic_comment\"]],\n",
    "        padding='max_length', \n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    target_encoding = tokenizer(\n",
    "        [f'{i}</s>' for i in examples['neutral_comment1']],\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    labels = target_encoding.input_ids\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    tokenized_examples['labels'] = labels\n",
    "    return tokenized_examples\n",
    "\n",
    "def prepare_features_test(examples):\n",
    "    tokenized_examples = tokenizer(\n",
    "        ['<LM>' + x for x in examples[\"toxic_comment\"]], \n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6948 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(prepare_features, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=DEVICE_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    'models/fredt5large',\n",
    "    evaluation_strategy = 'epoch',\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 4,\n",
    "    learning_rate = 1e-4,\n",
    "    weight_decay=0.001,\n",
    "    num_train_epochs = 5,\n",
    "    save_strategy = 'epoch',\n",
    "    report_to = 'none',\n",
    "    gradient_accumulation_steps=1,\n",
    "    predict_with_generate = True,\n",
    "    save_total_limit = 1,\n",
    "    optim='adafactor',\n",
    "    logging_strategy='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset = dataset['train'],\n",
    "    eval_dataset = dataset['validation'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
